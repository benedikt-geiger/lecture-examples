{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# MINE Mutual Information Estimation in Zero-Dispersion Optical Channel \n",
    "\n",
    "This code is provided as supplementary material of the lecture Machine Learning and Optimization in Communications (MLOC).<br>\n",
    "\n",
    "This code illustrates\n",
    "* Using the MINE mutual information estimator [1] to compute the achievable information rate (AIR) of the channel\n",
    "\n",
    "[1] M. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D Hjelm, \"MINE: Mutual information\n",
    "neural estimation,\" _Proc. International Conference on Machine Learning (ICML)_, preprint available at https://arxiv.org/abs/1801.04062, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the following device for learning: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"We are using the following device for learning:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the parameters of the transmission as the fiber length $L$ (in km), the fiber nonlinearity coefficienty $\\gamma$ (given in 1/W/km) and the total noise power $P_n$ (given in dBM. The noise is due to amplified spontaneous emission in amplifiers along the link). We assume a model of a dispersion-less fiber affected by nonlinearity. The model, which is described for instance in [2] is given by an iterative application of the equation\n",
    "$$\n",
    "\\tilde{t}_{k+1} = \\tilde{t}_k\\exp\\left(\\jmath\\frac{L}{K}\\gamma|\\tilde{t}_k|^2\\right) + n_{k+1},\\qquad 0 \\leq k < K\n",
    "$$\n",
    "where $\\tilde{t}_0=t$ is the channel input (the modulated, complex symbols) and $r=\\tilde{t}_K$ is the channel output. $K$ denotes the number of steps taken to simulate the channel. Usually $K=50$ gives a good approximation.\n",
    "\n",
    "Here, we specify a 16-QAM to communicate over the channel.\n",
    "\n",
    "[2] S. Li, C. Häger, N. Garcia, and H. Wymeersch, \"Achievable Information Rates for Nonlinear Fiber Communication via End-to-end Autoencoder Learning,\" _Proc. European Conference on Optical Communications (ECOC)_, Rome, Sep. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of transmission (in km)\n",
    "L = 5000\n",
    "\n",
    "# fiber nonlinearity coefficient\n",
    "gamma = 1.27\n",
    "\n",
    "Pn = -21.3 # noise power (in dBm)\n",
    "\n",
    "Kstep = 10 # number of steps used in the channel model\n",
    "\n",
    "\n",
    "constellations = {'16-QAM': np.array([-3,-3,-3,-3,-1,-1,-1,-1,1,1,1,1,3,3,3,3]) + 1j*np.array([-3,-1,1,3,-3,-1,1,3,-3,-1,1,3,-3,-1,1,3]), \\\n",
    "                  '16-APSK': np.array([1,-1,0,0,1.4,1.4,-1.4,-1.4,3,-3,0,0,5,-5,0,0]) + 1j*np.array([0,0,1,-1,1.4,-1.4,1.4,-1.4,0,0,4,-4,0,0,6,-6])}\n",
    "\n",
    "def simulate_channel(x, Pin, constellation):      \n",
    "    # modulate 16-qam\n",
    "    assert all(x >= 0)\n",
    "    assert all(x < len(constellation))\n",
    "    # input power (in W), normalize to input power\n",
    "    input_power_linear = 10**((Pin-30)/10)\n",
    "    norm_factor = 1 / np.sqrt(np.mean(np.abs(constellation)**2)/input_power_linear)\n",
    "    modulated = constellation[x] * norm_factor\n",
    "\n",
    "    # noise variance per step    \n",
    "    sigma = np.sqrt((10**((Pn-30)/10)) / Kstep / 2)    \n",
    "\n",
    "    # channel model\n",
    "    temp = np.array(modulated, copy=True)\n",
    "    for i in range(Kstep):\n",
    "        power = np.absolute(temp)**2\n",
    "        rotcoff = (L / Kstep) * gamma * power\n",
    "        \n",
    "        temp = temp * np.exp(1j*rotcoff) + sigma*(np.random.randn(len(x)) + 1j*np.random.randn(len(x)))\n",
    "    return modulated,temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 16-QAM transmission over this channel.\n",
    "\n",
    "Show constellation as a function of the fiber input power. When the input power is small, the effect of the nonlinearity is small (as $\\jmath\\frac{L}{K}\\gamma|x_k|^2 \\approx 0$) and the transmission is dominated by the additive noise. If the input power becomes larger, the effect of the noise (the noise power is constant) becomes less pronounced, but the constellation rotates due to the larger input power and hence effect of the nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef45bee854944f65a262910aff5f96b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='Input Power Pin (dBm)', lay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_plot = 4000\n",
    "\n",
    "def plot_constellation(Pin, constellation_name):\n",
    "    constellation = constellations[constellation_name]\n",
    "        \n",
    "    t = np.random.randint(len(constellation),size=length_plot)\n",
    "    _,r = simulate_channel(t, Pin, constellation)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    font = {'size'   : 14}\n",
    "    plt.rc('font', **font)\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.subplot(1,2,1)\n",
    "    r_tx = constellation[range(len(constellation))]\n",
    "    plt.scatter(np.real(r_tx), np.imag(r_tx), c=range(len(constellation)), marker='o', s=200, cmap='tab20')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel(r'$\\Re\\{r\\}$',fontsize=14)\n",
    "    plt.ylabel(r'$\\Im\\{r\\}$',fontsize=14)\n",
    "    plt.title('Transmitted constellation')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(np.real(r), np.imag(r), c=t, cmap='tab20',s=4)\n",
    "    plt.xlabel(r'$\\Re\\{r\\}$',fontsize=14)\n",
    "    plt.ylabel(r'$\\Im\\{r\\}$',fontsize=14)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Received constellation ($L = %d$\\,km, $P_{in} = %1.2f$\\,dBm)' % (L, Pin))    \n",
    "    #plt.savefig('%s_received_zd_%1.2f.pdf' % (constellation_name.replace('-','_'),Pin),bbox_inches='tight')\n",
    "    \n",
    "interactive_update = interactive(plot_constellation, \\\n",
    "                                 Pin = widgets.FloatSlider(min=-10.0,max=10.0,step=0.1,value=1, continuous_update=False, description='Input Power Pin (dBm)', style={'description_width': 'initial'}, layout=widgets.Layout(width='50%')), \\\n",
    "                                 constellation_name = widgets.RadioButtons(options=['16-QAM','16-APSK'], value='16-QAM',continuous_update=False,description='Constellation'))\n",
    "\n",
    "\n",
    "output = interactive_update.children[-1]\n",
    "output.layout.height = '400px'\n",
    "interactive_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MINE estimator [1] uses the Donsker-Varadhan variational represention of the mutual information, which is given by\n",
    "$$\n",
    "I(\\mathsf{t};\\mathsf{r}) = \\sup_{f:\\mathcal{T}\\times\\mathcal{R}\\to\\mathbb{R}}\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t},\\mathsf{r})}\\{f(\\mathsf{t},\\mathsf{r})\\} - \\log\\left(\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}\\right)\n",
    "$$\n",
    "where $f:\\mathcal{T}\\times\\mathcal{R}\\to\\mathbb{R}$ is any function that maps input and output of the channel to a scalar value. The maximization (\"$\\sup$\") needs to be carried out over all such functions. In this example, we use a neural network to approximate this variational function. The neural network is implemented in the next block and consists of three hidden layers with ELU activation function. We need to make sure that the function is numerically stable and therefore use an initializer for the initial weights with very small variance.\n",
    "\n",
    "[1] M. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D. Hjelm, \"MINE: Mutual information\n",
    "neural estimation,\" __Proc. ICML__, preprint available at \\url{https://arxiv.org/abs/1801.04062}, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variational_Function(nn.Module):\n",
    "    def __init__(self, hidden_neurons_1, hidden_neurons_2, hidden_neurons_3):\n",
    "        super(Variational_Function, self).__init__()\n",
    "        \n",
    "        # Linear function, 4 input neurons (real and imaginary part of transmit signal, real and imaginary part of receive signal)        \n",
    "        self.fc1 = nn.Linear(4, hidden_neurons_1) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.activation_function = nn.ELU()\n",
    "    \n",
    "        # Soft-sign\n",
    "        self.softsign = nn.Softsign()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        \n",
    "        # Linear function (hidden layer)\n",
    "        self.fc2 = nn.Linear(hidden_neurons_1, hidden_neurons_2)  \n",
    "\n",
    "        # Linear function (hidden layer)\n",
    "        self.fc3 = nn.Linear(hidden_neurons_1, hidden_neurons_3)  \n",
    "\n",
    "        # Output layer generating a scalar\n",
    "        self.fc4 = nn.Linear(hidden_neurons_3, 1)\n",
    "        \n",
    "        # initialize weights of layers\n",
    "        nn.init.normal_(self.fc1.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.normal_(self.fc2.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        nn.init.normal_(self.fc3.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc3.bias, 0)\n",
    "        nn.init.normal_(self.fc4.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc4.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear function, first layer\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity, first layer        \n",
    "        out = self.activation_function(out)\n",
    "        \n",
    "        # Linear function, second layer\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # Non-linearity, second layer        \n",
    "        out = self.activation_function(out)\n",
    "        \n",
    "        # Linear function, second layer\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        # Non-linearity, second layer        \n",
    "        out = self.activation_function(out)        \n",
    "        \n",
    "        # Linear function, output layer\n",
    "       \n",
    "        out = self.fc4(out)\n",
    "                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry out the final optimization. We use the Adam algorithm with small learning rate and use a modification of the cost function, which leads to a more stable estimator.\n",
    "\n",
    "Let $J(\\boldsymbol{t}, \\boldsymbol{r}, \\boldsymbol{\\theta})$ be the loss function. Then we exploit the fact that with\n",
    "$$\n",
    "J(\\boldsymbol{t}, \\boldsymbol{r}, \\boldsymbol{\\theta}) := \\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t},\\mathsf{r})}\\{f(\\mathsf{t},\\mathsf{r})\\} - \\log\\left(\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}\\right)\n",
    "$$ we have\n",
    "\\begin{align*}\n",
    "\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{t}, \\boldsymbol{r}, \\boldsymbol{\\theta}) &= \\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t},\\mathsf{r})}\\{f(\\mathsf{t},\\mathsf{r})\\} - \\frac{\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}}{\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}}\n",
    "\\end{align*}\n",
    "Applying stochastic gradient descent over the batch with $N$ examples yields\n",
    "\\begin{align*}\n",
    "\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{t}, \\boldsymbol{r}, \\boldsymbol{\\theta}) &\\approx \\frac{1}{N}\\sum_{i=1}^N\\nabla_{\\boldsymbol{\\theta}}f(t_i,r_i) - \\frac{\\frac{1}{N}\\sum_{i=1}^N\\nabla_{\\boldsymbol{\\theta}}\\mathrm{e}^{f(t_i,\\tilde{r}_i)}}{\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}}\n",
    "\\end{align*}\n",
    "where $r_i$ is the result of transmitting $t_i$ over the channel and $\\tilde{r}_i$ is the result of transmitting $\\tilde{t}_i$ over the channel, where we discard the transmit sequence $\\tilde{t}_i$. This needs to be done to ensure that the expectation is taken over the product of marginal distributions ($\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\{\\cdot\\}$). For details, see [1].\n",
    "\n",
    "In the denominator of the approximated gradient, we have the term $\\mathbb{E}_{\\mathsf{t},\\mathsf{r}\\sim p(\\mathsf{t})p(\\mathsf{r})}\\left\\{\\mathrm{e}^{f(\\mathsf{t},\\mathsf{r})}\\right\\}$. We estimate this term using a moving average over the mini-batches. We need to make sure that this term is not taken into account when calculating the gradient, which is done using the `.detach()` function. The original loss function is also available in the code below as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: current MI estimate is 0.000 bit/channel use\n",
      "Epoch 1: current MI estimate is 0.000 bit/channel use\n",
      "Epoch 2: current MI estimate is 0.021 bit/channel use\n",
      "Epoch 3: current MI estimate is 0.053 bit/channel use\n",
      "Epoch 4: current MI estimate is 0.059 bit/channel use\n",
      "Epoch 5: current MI estimate is 0.075 bit/channel use\n",
      "Epoch 6: current MI estimate is 0.252 bit/channel use\n",
      "Epoch 7: current MI estimate is 0.515 bit/channel use\n",
      "Epoch 8: current MI estimate is 0.756 bit/channel use\n",
      "Epoch 9: current MI estimate is 0.867 bit/channel use\n",
      "Epoch 10: current MI estimate is 1.038 bit/channel use\n",
      "Epoch 11: current MI estimate is 1.290 bit/channel use\n",
      "Epoch 12: current MI estimate is 1.530 bit/channel use\n",
      "Epoch 13: current MI estimate is 1.727 bit/channel use\n",
      "Epoch 14: current MI estimate is 1.895 bit/channel use\n",
      "Epoch 15: current MI estimate is 1.968 bit/channel use\n",
      "Epoch 16: current MI estimate is 2.063 bit/channel use\n",
      "Epoch 17: current MI estimate is 2.132 bit/channel use\n",
      "Epoch 18: current MI estimate is 2.163 bit/channel use\n",
      "Epoch 19: current MI estimate is 2.211 bit/channel use\n",
      "Epoch 20: current MI estimate is 2.248 bit/channel use\n",
      "Epoch 21: current MI estimate is 2.273 bit/channel use\n",
      "Epoch 22: current MI estimate is 2.308 bit/channel use\n",
      "Epoch 23: current MI estimate is 2.339 bit/channel use\n",
      "Epoch 24: current MI estimate is 2.387 bit/channel use\n",
      "Epoch 25: current MI estimate is 2.392 bit/channel use\n",
      "Epoch 26: current MI estimate is 2.428 bit/channel use\n",
      "Epoch 27: current MI estimate is 2.448 bit/channel use\n",
      "Epoch 28: current MI estimate is 2.435 bit/channel use\n",
      "Epoch 29: current MI estimate is 2.494 bit/channel use\n",
      "Epoch 30: current MI estimate is 2.511 bit/channel use\n",
      "Epoch 31: current MI estimate is 2.508 bit/channel use\n",
      "Epoch 32: current MI estimate is 2.540 bit/channel use\n",
      "Epoch 33: current MI estimate is 2.553 bit/channel use\n",
      "Epoch 34: current MI estimate is 2.571 bit/channel use\n",
      "Epoch 35: current MI estimate is 2.567 bit/channel use\n",
      "Epoch 36: current MI estimate is 2.597 bit/channel use\n",
      "Epoch 37: current MI estimate is 2.610 bit/channel use\n",
      "Epoch 38: current MI estimate is 2.633 bit/channel use\n",
      "Epoch 39: current MI estimate is 2.632 bit/channel use\n",
      "Epoch 40: current MI estimate is 2.663 bit/channel use\n",
      "Epoch 41: current MI estimate is 2.667 bit/channel use\n",
      "Epoch 42: current MI estimate is 2.693 bit/channel use\n",
      "Epoch 43: current MI estimate is 2.675 bit/channel use\n",
      "Epoch 44: current MI estimate is 2.704 bit/channel use\n",
      "Epoch 45: current MI estimate is 2.700 bit/channel use\n",
      "Epoch 46: current MI estimate is 2.746 bit/channel use\n",
      "Epoch 47: current MI estimate is 2.725 bit/channel use\n",
      "Epoch 48: current MI estimate is 2.764 bit/channel use\n",
      "Epoch 49: current MI estimate is 2.753 bit/channel use\n",
      "Epoch 50: current MI estimate is 2.776 bit/channel use\n",
      "Epoch 51: current MI estimate is 2.771 bit/channel use\n",
      "Epoch 52: current MI estimate is 2.796 bit/channel use\n",
      "Epoch 53: current MI estimate is 2.769 bit/channel use\n",
      "Epoch 54: current MI estimate is 2.816 bit/channel use\n",
      "Epoch 55: current MI estimate is 2.788 bit/channel use\n",
      "Epoch 56: current MI estimate is 2.802 bit/channel use\n",
      "Epoch 57: current MI estimate is 2.814 bit/channel use\n",
      "Epoch 58: current MI estimate is 2.811 bit/channel use\n",
      "Epoch 59: current MI estimate is 2.838 bit/channel use\n",
      "Epoch 60: current MI estimate is 2.824 bit/channel use\n",
      "Epoch 61: current MI estimate is 2.843 bit/channel use\n",
      "Epoch 62: current MI estimate is 2.856 bit/channel use\n",
      "Epoch 63: current MI estimate is 2.861 bit/channel use\n",
      "Epoch 64: current MI estimate is 2.868 bit/channel use\n",
      "Epoch 65: current MI estimate is 2.855 bit/channel use\n",
      "Epoch 66: current MI estimate is 2.875 bit/channel use\n",
      "Epoch 67: current MI estimate is 2.891 bit/channel use\n",
      "Epoch 68: current MI estimate is 2.898 bit/channel use\n",
      "Epoch 69: current MI estimate is 2.901 bit/channel use\n",
      "Epoch 70: current MI estimate is 2.897 bit/channel use\n",
      "Epoch 71: current MI estimate is 2.900 bit/channel use\n",
      "Epoch 72: current MI estimate is 2.917 bit/channel use\n",
      "Epoch 73: current MI estimate is 2.927 bit/channel use\n",
      "Epoch 74: current MI estimate is 2.935 bit/channel use\n",
      "Epoch 75: current MI estimate is 2.893 bit/channel use\n",
      "Epoch 76: current MI estimate is 2.958 bit/channel use\n",
      "Epoch 77: current MI estimate is 2.951 bit/channel use\n",
      "Epoch 78: current MI estimate is 2.949 bit/channel use\n",
      "Epoch 79: current MI estimate is 2.965 bit/channel use\n",
      "Epoch 80: current MI estimate is 2.954 bit/channel use\n",
      "Epoch 81: current MI estimate is 2.954 bit/channel use\n",
      "Epoch 82: current MI estimate is 2.982 bit/channel use\n",
      "Epoch 83: current MI estimate is 2.975 bit/channel use\n",
      "Epoch 84: current MI estimate is 2.978 bit/channel use\n",
      "Epoch 85: current MI estimate is 2.985 bit/channel use\n",
      "Epoch 86: current MI estimate is 2.976 bit/channel use\n",
      "Epoch 87: current MI estimate is 2.999 bit/channel use\n",
      "Epoch 88: current MI estimate is 2.982 bit/channel use\n",
      "Epoch 89: current MI estimate is 2.983 bit/channel use\n",
      "Epoch 90: current MI estimate is 3.011 bit/channel use\n",
      "Epoch 91: current MI estimate is 2.999 bit/channel use\n",
      "Epoch 92: current MI estimate is 3.003 bit/channel use\n",
      "Epoch 93: current MI estimate is 3.027 bit/channel use\n",
      "Epoch 94: current MI estimate is 3.032 bit/channel use\n",
      "Epoch 95: current MI estimate is 3.040 bit/channel use\n",
      "Epoch 96: current MI estimate is 3.008 bit/channel use\n",
      "Epoch 97: current MI estimate is 3.034 bit/channel use\n",
      "Epoch 98: current MI estimate is 3.048 bit/channel use\n",
      "Epoch 99: current MI estimate is 3.056 bit/channel use\n"
     ]
    }
   ],
   "source": [
    "Pin = 3\n",
    "constellation = constellations['16-QAM']\n",
    "\n",
    "hidden_neurons_1 = 100\n",
    "hidden_neurons_2 = 100\n",
    "hidden_neurons_3 = 100\n",
    "\n",
    "\n",
    "# Generate variational function\n",
    "model = Variational_Function(hidden_neurons_1, hidden_neurons_2, hidden_neurons_3)\n",
    "model.to(device)\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  \n",
    "\n",
    "num_epochs = 100\n",
    "batches_per_epoch = 1000\n",
    "\n",
    "# increase batch size while learning from 500 up to 2000\n",
    "batch_size_per_epoch = np.linspace(1000, 30000, num=num_epochs, dtype=int)\n",
    "\n",
    "ma_et = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step in range(batches_per_epoch):\n",
    "        \n",
    "        ti = np.random.randint(len(constellation), size=batch_size_per_epoch[epoch])\n",
    "        t,r = simulate_channel(ti, Pin, constellation)\n",
    "\n",
    "        ti = np.random.randint(len(constellation), size=batch_size_per_epoch[epoch])\n",
    "        _,rtilde = simulate_channel(ti, Pin, constellation)\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "                \n",
    "        # generate input to first call of the variational function\n",
    "        input_1 = np.vstack( (np.real(t), np.imag(t), np.real(r), np.imag(r)) ).transpose()\n",
    "        torch_in1 = torch.from_numpy(input_1).float().to(device)\n",
    "        \n",
    "        # generate input to second call of the variational function\n",
    "        input_2 = np.vstack( (np.real(t), np.imag(t), np.real(rtilde), np.imag(rtilde)) ).transpose()\n",
    "        torch_in2 = torch.from_numpy(input_2).float().to(device)\n",
    "        \n",
    "        # apply the variational function\n",
    "        out_1 = model(torch_in1)\n",
    "        out_2 = model(torch_in2)\n",
    "        \n",
    "        \n",
    "        # loss function. Attenation, put a minus sign as we want to maximize        \n",
    "        #loss = -torch.mean(out_1) + torch.log(torch.mean(torch.exp(out_2)))        \n",
    "\n",
    "        # biased-corrected estimator with moving average\n",
    "        tmp = torch.exp(out_2)\n",
    "        ma_et = (1.0-0.05)*ma_et + 0.05*torch.mean(tmp.detach())                 \n",
    "        loss = -torch.mean(out_1) + (1/ma_et.mean()).detach() * torch.mean(torch.exp(out_2))        \n",
    "        \n",
    "              \n",
    "        # compute gradients\n",
    "        loss.backward() \n",
    "        \n",
    "        # carry out one optimization step with Adam\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "    #print(loss)\n",
    "    model.eval()\n",
    "    ti = np.random.randint(len(constellation),size=100000)\n",
    "    t,r = simulate_channel(ti, Pin, constellation)\n",
    "        \n",
    "    rtilde = r[np.random.permutation(len(r))]\n",
    "        \n",
    "    # generate input to first call of the variational function\n",
    "    input_1 = np.vstack( (np.real(t), np.imag(t), np.real(r), np.imag(r)) ).transpose()\n",
    "    torch_in1 = torch.from_numpy(input_1).float().to(device)\n",
    "        \n",
    "    # generate input to second call of the variational function\n",
    "    input_2 = np.vstack( (np.real(t), np.imag(t), np.real(rtilde), np.imag(rtilde)) ).transpose()\n",
    "    torch_in2 = torch.from_numpy(input_2).float().to(device)\n",
    "        \n",
    "    # apply the variational function\n",
    "    out_1 = model(torch_in1)\n",
    "    out_2 = model(torch_in2)\n",
    "        \n",
    "    # loss function. Attenation, put a minus sign as we want to maximize    \n",
    "    MI_eval = torch.mean(out_1) - torch.log2(torch.mean(torch.pow(2.0, out_2)))    \n",
    "    print('Epoch %d: current MI estimate is %1.3f bit/channel use' % (epoch, MI_eval.cpu().detach().numpy()) )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
